{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rg_YV9IoJdgt",
        "outputId": "c6386278-a7cc-49ab-b636-540caff33f72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-jz9o1vdd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-jz9o1vdd\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 31bebdea147c96f8a00a0d55931858bf727ae370\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.4.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.4)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.17.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.1)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.2.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.12.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.5)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=6174583 sha256=a23f23987380b8e4e7de0ad0e182b334393eaff5e9a0043b81c2b94586cbc195\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x716uxsr/wheels/47/e5/15/94c80df2ba85500c5d76599cc307c0a7079d0e221bb6fc4375\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61398 sha256=3aab4b91c8aa9303f02cea38cabca929c8c64acf88fddb3452252ac8b52945dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144552 sha256=c817bccddb2b203dbcf90f6f3e235527e7c78a652e9a53a85f0a6b7f49a75545\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-24.8.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-2.10.1 yacs-0.1.8\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "7e9218d70c6c4cef8603a9bd5f74f618",
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install git+https://github.com/facebookresearch/detectron2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zehIjzlEjDFq",
        "outputId": "741f5da3-3c9c-402b-a064-fac486d8c019"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Segmentation complete. Output saved as 'segmented_image.png'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "from torchvision.transforms import functional as F\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def load_model():\n",
        "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = F.to_tensor(image)\n",
        "    return image_tensor.unsqueeze(0)\n",
        "\n",
        "def segment_image(model, image_tensor):\n",
        "    with torch.no_grad():\n",
        "        prediction = model(image_tensor)[0]\n",
        "    return prediction\n",
        "\n",
        "def visualize_segmentation(image, masks, scores, threshold=0.5):\n",
        "    image = image.squeeze().permute(1, 2, 0).numpy()\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(image)\n",
        "\n",
        "    for mask, score in zip(masks, scores):\n",
        "        if score > threshold:\n",
        "            masked = np.where(mask.squeeze().numpy() > 0.5, 1, 0)\n",
        "            plt.contour(masked, colors=['red'], alpha=0.5, linewidths=2)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('segmented_image.png')\n",
        "    plt.close()\n",
        "\n",
        "def main(image_path):\n",
        "    model = load_model()\n",
        "    image_tensor = preprocess_image(image_path)\n",
        "    prediction = segment_image(model, image_tensor)\n",
        "\n",
        "    masks = prediction['masks']\n",
        "    scores = prediction['scores']\n",
        "\n",
        "    visualize_segmentation(image_tensor, masks, scores)\n",
        "    print(f\"Segmentation complete. Output saved as 'segmented_image.png'\")\n",
        "    return masks, scores\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    image_path = \"/content/WhatsApp Image 2024-02-27 at 3.07.59 PM (2).jpeg\"\n",
        "    main(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJDgzoY36cBk",
        "outputId": "8ae36b7b-e187-422c-cdac-d549d3acc5df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 11 objects. Master ID: 8f07c123-bbf2-4630-b9ea-951ef1958f02\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import sqlite3\n",
        "import os\n",
        "import uuid\n",
        "import torch\n",
        "\n",
        "def extract_objects(image_path, masks, scores, threshold=0.5):\n",
        "    # Load the original image\n",
        "    image = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    extracted_objects = []\n",
        "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "        if score > threshold:\n",
        "            # Convert mask to binary\n",
        "            binary_mask = (mask.squeeze().numpy() > 0.5).astype(np.uint8) * 255\n",
        "\n",
        "            # Find contours\n",
        "            contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            # Create a mask for the largest contour\n",
        "            object_mask = np.zeros(binary_mask.shape, dtype=np.uint8)\n",
        "            cv2.drawContours(object_mask, contours, -1, (255), thickness=cv2.FILLED)\n",
        "\n",
        "            # Extract the object\n",
        "            extracted_object = cv2.bitwise_and(image_rgb, image_rgb, mask=object_mask)\n",
        "\n",
        "            # Crop the object to its bounding box\n",
        "            x, y, w, h = cv2.boundingRect(object_mask)\n",
        "            cropped_object = extracted_object[y:y+h, x:x+w]\n",
        "\n",
        "            extracted_objects.append(cropped_object)\n",
        "\n",
        "    return extracted_objects\n",
        "\n",
        "def save_objects(extracted_objects, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    object_ids = []\n",
        "    for i, obj in enumerate(extracted_objects):\n",
        "        obj_id = str(uuid.uuid4())  # Generate a unique ID\n",
        "        object_ids.append(obj_id)\n",
        "\n",
        "        # Convert from BGR to RGB\n",
        "        obj_rgb = cv2.cvtColor(obj, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Save the object as an image\n",
        "        img = Image.fromarray(obj_rgb)\n",
        "        img.save(os.path.join(output_folder, f\"{obj_id}.png\"))\n",
        "\n",
        "    return object_ids\n",
        "\n",
        "def create_database():\n",
        "    conn = sqlite3.connect('objects_database.db')\n",
        "    c = conn.cursor()\n",
        "    c.execute('''CREATE TABLE IF NOT EXISTS objects\n",
        "                 (id TEXT PRIMARY KEY, master_id TEXT)''')\n",
        "    conn.commit()\n",
        "    return conn\n",
        "\n",
        "def store_metadata(conn, object_ids, master_id):\n",
        "    c = conn.cursor()\n",
        "    for obj_id in object_ids:\n",
        "        c.execute(\"INSERT INTO objects (id, master_id) VALUES (?, ?)\", (obj_id, master_id))\n",
        "    conn.commit()\n",
        "\n",
        "def main(image_path, model):\n",
        "    # Use the model to get masks and scores\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = torch.from_numpy(np.array(image).transpose((2, 0, 1))).float().unsqueeze(0) / 255.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = model(image_tensor)[0]\n",
        "\n",
        "    masks = prediction['masks']\n",
        "    scores = prediction['scores']\n",
        "\n",
        "    # Extract objects\n",
        "    extracted_objects = extract_objects(image_path, masks, scores)\n",
        "\n",
        "    # Save objects and get their IDs\n",
        "    output_folder = \"extracted_objects\"\n",
        "    object_ids = save_objects(extracted_objects, output_folder)\n",
        "\n",
        "    # Generate a master ID for the original image\n",
        "    master_id = str(uuid.uuid4())\n",
        "\n",
        "    # Store metadata in the database\n",
        "    conn = create_database()\n",
        "    store_metadata(conn, object_ids, master_id)\n",
        "    conn.close()\n",
        "\n",
        "    print(f\"Extracted {len(object_ids)} objects. Master ID: {master_id}\")\n",
        "    return object_ids, master_id\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "\n",
        "    image_path = \"/content/WhatsApp Image 2024-02-27 at 3.07.59 PM (2).jpeg\"\n",
        "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
        "    model.eval()\n",
        "\n",
        "    main(image_path, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhYTg3EW8ZsF",
        "outputId": "220aa8fe-d42e-45a2-ec01-328af450a990"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:03<00:00, 92.8MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Object identification complete. Descriptions saved to object_descriptions.json\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import clip\n",
        "import os\n",
        "import json\n",
        "\n",
        "def load_clip_model():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "    return model, preprocess, device\n",
        "\n",
        "def identify_object(model, preprocess, image_path, device):\n",
        "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "\n",
        "    # List of potential object categories\n",
        "    categories = [\n",
        "        \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "        \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
        "        \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\",\n",
        "        \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\",\n",
        "        \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\n",
        "        \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "        \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
        "        \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\",\n",
        "        \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\",\n",
        "        \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
        "    ]\n",
        "\n",
        "    text = clip.tokenize(categories).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        text_features = model.encode_text(text)\n",
        "\n",
        "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "        values, indices = similarity[0].topk(5)\n",
        "\n",
        "    results = [\n",
        "        {\"category\": categories[idx], \"confidence\": value.item()}\n",
        "        for value, idx in zip(values, indices)\n",
        "    ]\n",
        "\n",
        "    # Generate a description\n",
        "    top_category = results[0][\"category\"]\n",
        "    description = f\"This image appears to contain a {top_category}. \"\n",
        "    if len(results) > 1:\n",
        "        description += f\"It might also be a {results[1]['category']} or a {results[2]['category']}.\"\n",
        "\n",
        "    return results, description\n",
        "\n",
        "def process_objects(input_folder, model, preprocess, device):\n",
        "    object_descriptions = {}\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".png\"):\n",
        "            object_id = filename[:-4]  # Remove .png extension\n",
        "            image_path = os.path.join(input_folder, filename)\n",
        "\n",
        "            results, description = identify_object(model, preprocess, image_path, device)\n",
        "\n",
        "            object_descriptions[object_id] = {\n",
        "                \"top_categories\": results,\n",
        "                \"description\": description\n",
        "            }\n",
        "\n",
        "    return object_descriptions\n",
        "\n",
        "def save_descriptions(object_descriptions, output_file):\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(object_descriptions, f, indent=2)\n",
        "\n",
        "def main(input_folder):\n",
        "    model, preprocess, device = load_clip_model()\n",
        "    object_descriptions = process_objects(input_folder, model, preprocess, device)\n",
        "\n",
        "    output_file = \"object_descriptions.json\"\n",
        "    save_descriptions(object_descriptions, output_file)\n",
        "\n",
        "    print(f\"Object identification complete. Descriptions saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_folder = \"extracted_objects\"  # Folder containing extracted object images from Step 2\n",
        "    main(input_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aBCdxjH8qlt",
        "outputId": "87d3b99b-4760-4b45-fcd2-4c91ec5dc014"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% CompleteText extraction complete. Data saved to object_text_data.json\n"
          ]
        }
      ],
      "source": [
        "import easyocr\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def load_ocr_reader(languages=['en']):\n",
        "    return easyocr.Reader(languages)\n",
        "\n",
        "def extract_text(reader, image_path):\n",
        "    # Read the image\n",
        "    image = Image.open(image_path)\n",
        "    image_np = np.array(image)\n",
        "\n",
        "    # Perform OCR\n",
        "    results = reader.readtext(image_np)\n",
        "\n",
        "    # Extract text and confidence\n",
        "    extracted_data = [\n",
        "        {\n",
        "            \"text\": result[1],\n",
        "            \"confidence\": result[2],\n",
        "            \"bounding_box\": result[0]\n",
        "        } for result in results\n",
        "    ]\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def process_objects(input_folder, reader):\n",
        "    object_text_data = {}\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
        "            object_id = os.path.splitext(filename)[0]\n",
        "            image_path = os.path.join(input_folder, filename)\n",
        "\n",
        "            extracted_data = extract_text(reader, image_path)\n",
        "\n",
        "            object_text_data[object_id] = extracted_data\n",
        "\n",
        "    return object_text_data\n",
        "\n",
        "def save_text_data(object_text_data, output_file):\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(object_text_data, f, indent=2)\n",
        "\n",
        "def main(input_folder):\n",
        "    # Load the OCR reader\n",
        "    reader = load_ocr_reader()\n",
        "\n",
        "    # Process all objects\n",
        "    object_text_data = process_objects(input_folder, reader)\n",
        "\n",
        "    # Save the extracted text data\n",
        "    output_file = \"object_text_data.json\"\n",
        "    save_text_data(object_text_data, output_file)\n",
        "\n",
        "    print(f\"Text extraction complete. Data saved to {output_file}\")\n",
        "\n",
        "    return object_text_data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_folder = \"extracted_objects\"  # Folder containing extracted object images from Step 2\n",
        "    main(input_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpKLHr9j9BKD",
        "outputId": "a2bb68a1-3815-4965-b1bc-2cf827691624"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Object summarization complete. Summaries saved to object_summaries.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def load_previous_data(identification_file, text_data_file):\n",
        "    with open(identification_file, 'r') as f:\n",
        "        identification_data = json.load(f)\n",
        "\n",
        "    with open(text_data_file, 'r') as f:\n",
        "        text_data = json.load(f)\n",
        "\n",
        "    return identification_data, text_data\n",
        "\n",
        "def extract_key_terms(text, n=5):\n",
        "    # Tokenize and remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [w for w in word_tokens if w.isalnum() and w not in stop_words]\n",
        "\n",
        "    # Count and return top N most common terms\n",
        "    return [word for word, _ in Counter(filtered_tokens).most_common(n)]\n",
        "\n",
        "def generate_summary(object_id, identification, text_data):\n",
        "    top_category = identification['top_categories'][0]['category']\n",
        "    confidence = identification['top_categories'][0]['confidence']\n",
        "\n",
        "    summary = f\"Object {object_id} is identified as a {top_category} with {confidence:.2f} confidence. \"\n",
        "\n",
        "    if len(identification['top_categories']) > 1:\n",
        "        second_category = identification['top_categories'][1]['category']\n",
        "        summary += f\"It might also be a {second_category}. \"\n",
        "\n",
        "    if text_data:\n",
        "        extracted_text = ' '.join([item['text'] for item in text_data])\n",
        "        key_terms = extract_key_terms(extracted_text)\n",
        "        if key_terms:\n",
        "            summary += f\"Key terms associated with this object are: {', '.join(key_terms)}. \"\n",
        "    else:\n",
        "        summary += \"No text was extracted from this object. \"\n",
        "\n",
        "    return summary.strip()\n",
        "\n",
        "def process_objects(identification_data, text_data):\n",
        "    object_summaries = {}\n",
        "\n",
        "    for object_id in identification_data.keys():\n",
        "        identification = identification_data[object_id]\n",
        "        object_text_data = text_data.get(object_id, [])\n",
        "\n",
        "        summary = generate_summary(object_id, identification, object_text_data)\n",
        "        object_summaries[object_id] = summary\n",
        "\n",
        "    return object_summaries\n",
        "\n",
        "def save_summaries(object_summaries, output_file):\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(object_summaries, f, indent=2)\n",
        "\n",
        "def main(identification_file, text_data_file):\n",
        "    # Load data from previous steps\n",
        "    identification_data, text_data = load_previous_data(identification_file, text_data_file)\n",
        "\n",
        "    # Process all objects\n",
        "    object_summaries = process_objects(identification_data, text_data)\n",
        "\n",
        "    # Save the summaries\n",
        "    output_file = \"object_summaries.json\"\n",
        "    save_summaries(object_summaries, output_file)\n",
        "\n",
        "    print(f\"Object summarization complete. Summaries saved to {output_file}\")\n",
        "\n",
        "    return object_summaries\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    identification_file = \"object_descriptions.json\"  # Output from Step 3\n",
        "    text_data_file = \"object_text_data.json\"  # Output from Step 4\n",
        "    main(identification_file, text_data_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccbvByMa-fxV",
        "outputId": "cd91d125-8626-4b64-f5d3-92836cf0c22f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data mapping complete. Mapped data saved to mapped_data.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import sqlite3\n",
        "import os\n",
        "\n",
        "def load_data_from_previous_steps():\n",
        "    # Load object descriptions (Step 3)\n",
        "    with open('object_descriptions.json', 'r') as f:\n",
        "        object_descriptions = json.load(f)\n",
        "\n",
        "    # Load text data (Step 4)\n",
        "    with open('object_text_data.json', 'r') as f:\n",
        "        object_text_data = json.load(f)\n",
        "\n",
        "    # Load object summaries (Step 5)\n",
        "    with open('object_summaries.json', 'r') as f:\n",
        "        object_summaries = json.load(f)\n",
        "\n",
        "    return object_descriptions, object_text_data, object_summaries\n",
        "\n",
        "def get_object_metadata_from_db():\n",
        "    conn = sqlite3.connect('objects_database.db')\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT id, master_id FROM objects\")\n",
        "    object_metadata = {row[0]: {\"master_id\": row[1]} for row in c.fetchall()}\n",
        "    conn.close()\n",
        "    return object_metadata\n",
        "\n",
        "def map_data(object_metadata, object_descriptions, object_text_data, object_summaries):\n",
        "    mapped_data = {}\n",
        "\n",
        "    for object_id, metadata in object_metadata.items():\n",
        "        master_id = metadata['master_id']\n",
        "\n",
        "        if master_id not in mapped_data:\n",
        "            mapped_data[master_id] = {\n",
        "                \"objects\": {}\n",
        "            }\n",
        "\n",
        "        mapped_data[master_id][\"objects\"][object_id] = {\n",
        "            \"identification\": object_descriptions.get(object_id, {}),\n",
        "            \"extracted_text\": object_text_data.get(object_id, []),\n",
        "            \"summary\": object_summaries.get(object_id, \"\")\n",
        "        }\n",
        "\n",
        "    return mapped_data\n",
        "\n",
        "def save_mapped_data(mapped_data, output_file):\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(mapped_data, f, indent=2)\n",
        "\n",
        "def main():\n",
        "    # Load data from previous steps\n",
        "    object_descriptions, object_text_data, object_summaries = load_data_from_previous_steps()\n",
        "\n",
        "    # Get object metadata from the database\n",
        "    object_metadata = get_object_metadata_from_db()\n",
        "\n",
        "    # Map all data\n",
        "    mapped_data = map_data(object_metadata, object_descriptions, object_text_data, object_summaries)\n",
        "\n",
        "    # Save the mapped data\n",
        "    output_file = \"mapped_data.json\"\n",
        "    save_mapped_data(mapped_data, output_file)\n",
        "\n",
        "    print(f\"Data mapping complete. Mapped data saved to {output_file}\")\n",
        "\n",
        "    return mapped_data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eNFwiKACMBP",
        "outputId": "647a3fb5-5cda-4f7f-aed5-bc85d95c700b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output generated for master image 8f07c123-bbf2-4630-b9ea-951ef1958f02:\n",
            "- Annotated image saved as annotated_image_8f07c123-bbf2-4630-b9ea-951ef1958f02.png\n",
            "- Summary table saved as summary_table_8f07c123-bbf2-4630-b9ea-951ef1958f02.csv\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def load_mapped_data(mapped_data_file):\n",
        "    with open(mapped_data_file, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_image(image_path):\n",
        "    return cv2.imread(image_path)\n",
        "\n",
        "def annotate_image(image, objects_data):\n",
        "    # Convert BGR to RGB\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    pil_image = Image.fromarray(image_rgb)\n",
        "    draw = ImageDraw.Draw(pil_image)\n",
        "\n",
        "    # Use a default font\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "    for obj_id, obj_data in objects_data['objects'].items():\n",
        "        # Assuming bounding box information is available in obj_data\n",
        "        # If not, you might need to adjust this part\n",
        "        bbox = obj_data.get('bounding_box', [0, 0, 100, 100])  # default values if not available\n",
        "        x, y, w, h = bbox\n",
        "\n",
        "        # Draw bounding box\n",
        "        draw.rectangle([x, y, x+w, y+h], outline=\"red\", width=2)\n",
        "\n",
        "        # Draw label\n",
        "        label = obj_data['identification']['top_categories'][0]['category']\n",
        "        draw.text((x, y-20), f\"{label} ({obj_id})\", font=font, fill=\"red\")\n",
        "\n",
        "    return np.array(pil_image)\n",
        "\n",
        "def create_summary_table(objects_data):\n",
        "    rows = []\n",
        "    for obj_id, obj_data in objects_data['objects'].items():\n",
        "        row = {\n",
        "            'Object ID': obj_id,\n",
        "            'Category': obj_data['identification']['top_categories'][0]['category'],\n",
        "            'Confidence': obj_data['identification']['top_categories'][0]['confidence'],\n",
        "            'Extracted Text': '; '.join([text['text'] for text in obj_data['extracted_text']]),\n",
        "            'Summary': obj_data['summary']\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def save_output(annotated_image, summary_table, output_image_path, output_table_path):\n",
        "    # Save annotated image\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(annotated_image)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_image_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Save summary table\n",
        "    summary_table.to_csv(output_table_path, index=False)\n",
        "\n",
        "def main(mapped_data_file, original_image_path):\n",
        "    # Load mapped data\n",
        "    mapped_data = load_mapped_data(mapped_data_file)\n",
        "\n",
        "    # Process each master image\n",
        "    for master_id, master_data in mapped_data.items():\n",
        "        # Load original image\n",
        "        original_image = load_image(original_image_path)\n",
        "\n",
        "        # Annotate image\n",
        "        annotated_image = annotate_image(original_image, master_data)\n",
        "\n",
        "        # Create summary table\n",
        "        summary_table = create_summary_table(master_data)\n",
        "\n",
        "        # Save outputs\n",
        "        output_image_path = f\"annotated_image_{master_id}.png\"\n",
        "        output_table_path = f\"summary_table_{master_id}.csv\"\n",
        "        save_output(annotated_image, summary_table, output_image_path, output_table_path)\n",
        "\n",
        "        print(f\"Output generated for master image {master_id}:\")\n",
        "        print(f\"- Annotated image saved as {output_image_path}\")\n",
        "        print(f\"- Summary table saved as {output_table_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mapped_data_file = \"mapped_data.json\"\n",
        "    original_image_path = \"/content/WhatsApp Image 2024-02-27 at 3.07.59 PM (2).jpeg\"  # Replace with actual path\n",
        "    main(mapped_data_file, original_image_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
